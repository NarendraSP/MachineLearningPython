{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPAM DETECTION USING SVM (Support Vector Machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io \n",
    "from sklearn import svm\n",
    "import re \n",
    "import nltk, nltk.stem.porter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Sample Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emailSample1.txt:\n",
      "> Anyone knows how much it costs to host a web portal ?\n",
      ">\n",
      "Well, it depends on how many visitors you're expecting.\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
      "if youre running something big..\n",
      "\n",
      "To unsubscribe yourself from this mailing list, send an email to:\n",
      "groupname-unsubscribe@egroups.com\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"emailSample1.txt:\")\n",
    "print(open('emailSample1.txt', 'r' ).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting on a machine learning task, it is usually insightful to\n",
    "take a look at examples from the dataset.\n",
    "\n",
    "Above shows a sample email that contains a URL, an email address (at the end), numbers, and dollar amounts. While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the specific entities (e.g.,\n",
    "the specific __URL__ or specific dollar amount) will be different in almost every email. Therefore, one method often employed in processing emails is to \"normalize\" these values, so that all __URLs__ are treated the same, all numbers are treated the same, etc. \n",
    "\n",
    "For example, we could replace each __URL__ in the email with the unique string \"httpaddr\" to indicate that a __URL__ was present.\n",
    "\n",
    "This has the effect of letting the spam classiffier make a classiffication decision based on whether any __URL__ was present, rather than whether a specific URL was present. This typically improves the performance of a spam classiffier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small.\n",
    "\n",
    "- __Lower-casing__: The entire email is converted into lower case, so that captialization is ignored (e.g., __IndIcaTE__ is treated the same as __Indicate__).\n",
    "\n",
    "- __Stripping HTML__: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "\n",
    "- __Normalizing URLs__: All URLs are replaced with the text \"__httpaddr__\".\n",
    "\n",
    "- __Normalizing Email Addresses__: All email addresses are replaced with the text \"__emailaddr__\".\n",
    "\n",
    "- __Normalizing Numbers__: All numbers are replaced with the text \"__number__\".\n",
    "\n",
    "- __Normalizing Dollars__: All dollar signs are replaced with the text \"__dollar__\".\n",
    "\n",
    "- __Word Stemming__: Words are reduced to their stemmed form. For example, \"discount\", \"discounts\", \"discounted\" and \"discounting\" are all replaced with \"discount\". Sometimes, the Stemmer actually strips of additional characters from the end, so \"include\", \"includes\", \"included\", and \"including\" are all replaced with \"__includ__.\"\n",
    "\n",
    "- __Removal of non-words__: Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess( email ):\n",
    "    email = email.lower()\n",
    "    # Strip html tags. replace with a space\n",
    "    email = re.sub('<[^<>]+>', ' ', email);\n",
    "    #Any numbers get replaced with the string 'number'\n",
    "    email = re.sub('[0-9]+', 'number', email)\n",
    "    #Anything starting with http or https:// replaced with 'httpaddr'\n",
    "    email = re.sub('(http|https)://[^\\s]*', 'httpaddr', email)\n",
    "    #Strings with \"@\" in the middle are considered emails --> 'emailaddr'\n",
    "    email = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email);\n",
    "    #The '$' sign gets replaced with 'dollar'\n",
    "    email = re.sub('[$]+', 'dollar', email);\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2TokenList( raw_email ):\n",
    "    \"\"\"\n",
    "    Function that takes in preprocessed (simplified) email, tokenizes it,\n",
    "    stems each word, and returns an (ordered) list of tokens in the e-mail\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    email = preProcess( raw_email )\n",
    "    #Split the e-mail into individual words (tokens) (split by the delimiter ' ')\n",
    "    #Splitting by many delimiters is easiest with re.split()\n",
    "    tokens = re.split('[ \\@\\$\\/\\#\\.\\-\\:\\&\\*\\+\\=\\[\\]\\?\\!\\(\\)\\{\\}\\,\\'\\\"\\>\\_\\<\\;\\%]', email)\n",
    "    #Loop over each token and use a stemmer to shorten it, check if the word is in the vocab_list... if it is, store index\n",
    "    tokenlist = []\n",
    "    for token in tokens:\n",
    "        token = re.sub('[^a-zA-Z0-9]', '', token);\n",
    "        stemmed = stemmer.stem( token )\n",
    "        #Throw out empty tokens\n",
    "        if not len(token): continue\n",
    "        #Store a list of all unique stemmed words\n",
    "        tokenlist.append(stemmed)\n",
    "    return tokenlist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Vocabulary List\n",
    "After preprocessing the emails, we have a list of words for each email. The next step is to choose which words we would like to use in our classifier and which we would want to leave out.\n",
    "\n",
    "For this exercise, we have chosen only the most frequently occuring words as our set of words considered (the vocabulary list). Since words that occur rarely in the training set are only in a few emails, they might cause the model to overfit our training set. The complete vocabulary list is in the file vocab.txt. Our vocabulary list was selected by choosing all words which occur at least a 100 times in the spam corpus, resulting in a list of 1899 words. \n",
    "\n",
    "In practice, a vocabulary list with about 10,000 to 50,000 words is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabDict(reverse=False):\n",
    "    \"\"\"\n",
    "    Function to read in the supplied vocab list text file into a dictionary\n",
    "    Dictionary key is the stemmed word, value is the index in the text file\n",
    "    If \"reverse\", the keys and values are switched.\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    with open(\"vocab.txt\") as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            if not reverse:\n",
    "                vocab_dict[key] = int(val)\n",
    "            else:\n",
    "                vocab_dict[int(val)] = key              \n",
    "    return vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2VocabIndices( raw_email, vocab_dict ):\n",
    "    #returns a list of indices corresponding to the location in vocab_dict for each stemmed word \n",
    "    tokenlist = email2TokenList( raw_email )\n",
    "    index_list = [ vocab_dict[token] for token in tokenlist if token in vocab_dict ]\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extracting features from Emails\n",
    "\n",
    "We will now implement the feature extraction that converts each email into a vector in Rn. For this exercise, We will be using n = # words in vocabulary list. Specifically, the feature xi 2 f0; 1g for an email corresponds to whether the i-th word in the dictionary occurs in the email. That is, xi = 1 if the i-th word is in the email and xi = 0 if the i-th word is not present in the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2FeatureVector( raw_email, vocab_dict ):\n",
    "    # returns a vector of shape(n,1) where n is the size of the vocab_dict.\n",
    "    #he first element in this vector is 1 if the vocab word with index == 1 is in raw_email, else 0\n",
    "    n = len(vocab_dict)\n",
    "    result = np.zeros((n,1))\n",
    "    vocab_indices = email2VocabIndices( email_contents, vocab_dict )\n",
    "    for idx in vocab_indices:\n",
    "        result[idx] = 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector is 1899\n",
      "Number of non-zero entries is: 45\n"
     ]
    }
   ],
   "source": [
    "# the feature vector has length 1899 and 45 non-zero entries.\"\n",
    "\n",
    "vocab_dict = getVocabDict()\n",
    "email_contents = open( 'emailSample1.txt', 'r' ).read()\n",
    "test_fv = email2FeatureVector( email_contents, vocab_dict )\n",
    "\n",
    "print (\"Length of feature vector is %d\" % len(test_fv))\n",
    "print (\"Number of non-zero entries is: %d\" % sum(test_fv==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training SVM for Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training emails =  4000\n",
      "Number of training spam emails =  1277\n",
      "Number of training nonspam emails =  2723\n"
     ]
    }
   ],
   "source": [
    "#svm for spam classification\n",
    "datafile = 'spamTrain.mat'\n",
    "mat = scipy.io.loadmat( datafile )\n",
    "X, y = mat['X'], mat['y']\n",
    "# Test set\n",
    "datafile = 'spamTest.mat'\n",
    "mat = scipy.io.loadmat( datafile )\n",
    "Xtest, ytest = mat['Xtest'], mat['ytest']\n",
    "pos = np.array([X[i] for i in range(X.shape[0]) if y[i] == 1])\n",
    "neg = np.array([X[i] for i in range(X.shape[0]) if y[i] == 0])\n",
    "print ('Total number of training emails = ',X.shape[0])\n",
    "print ('Number of training spam emails = ',pos.shape[0])\n",
    "print ('Number of training nonspam emails = ',neg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we make an instance of an SVM with C=0.1 and 'linear' kernel\n",
    "linear_svm = svm.SVC(C=0.1, kernel='linear')\n",
    "\n",
    "# Now we fit the SVM to our X matrix, given the labels y\n",
    "linear_svm.fit( X, y.flatten() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 99.83%\n",
      "Test set accuracy = 98.90%\n"
     ]
    }
   ],
   "source": [
    "#  training accuracy of about 99.8% and a test accuracy of about 98.5%\"\n",
    "\n",
    "train_predictions = linear_svm.predict(X).reshape((y.shape[0],1))\n",
    "train_acc = 100. * float(sum(train_predictions == y))/y.shape[0]\n",
    "print ('Training accuracy = %0.2f%%' % train_acc)\n",
    "\n",
    "test_predictions = linear_svm.predict(Xtest).reshape((ytest.shape[0],1))\n",
    "test_acc = 100. * float(sum(test_predictions == ytest))/ytest.shape[0]\n",
    "print ('Test set accuracy = %0.2f%%' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Top Predictors for Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15 most important words to classify a spam e-mail are:\n",
      "['otherwis', 'clearli', 'remot', 'gt', 'visa', 'base', 'doesn', 'wife', 'previous', 'player', 'mortgag', 'natur', 'll', 'futur', 'hot']\n",
      "The 15 least important words to classify a spam e-mail are:\n",
      "['http', 'toll', 'xp', 'ratio', 'august', 'unsubscrib', 'useless', 'numberth', 'round', 'linux', 'datapow', 'wrong', 'urgent', 'that', 'spam']\n",
      "# of spam containing \"otherwis\" = 804/1277 = 62.96%\n",
      "# of NON spam containing \"otherwis\" = 301/2723 = 11.05%\n"
     ]
    }
   ],
   "source": [
    "# Determine the words most likely to indicate an e-mail is a spam\n",
    "# From the trained SVM we can get a list of the weight coefficients for each\n",
    "# word (technically, each word index)\n",
    "\n",
    "vocab_dict_flipped = getVocabDict(reverse=True)\n",
    "\n",
    "#Sort indicies from most important to least-important (high to low weight)\n",
    "sorted_indices = np.argsort( linear_svm.coef_, axis=None )[::-1]\n",
    "print (\"The 15 most important words to classify a spam e-mail are:\")\n",
    "print ([ vocab_dict_flipped[x] for x in sorted_indices[:15] ])\n",
    "print\n",
    "print (\"The 15 least important words to classify a spam e-mail are:\")\n",
    "print ([ vocab_dict_flipped[x] for x in sorted_indices[-15:] ])\n",
    "print\n",
    "\n",
    "# Most common word (mostly to debug):\n",
    "most_common_word = vocab_dict_flipped[sorted_indices[0]]\n",
    "print ('# of spam containing \\\"%s\\\" = %d/%d = %0.2f%%'% \\\n",
    "    (most_common_word, sum(pos[:,1190]),pos.shape[0],  \\\n",
    "     100.*float(sum(pos[:,1190]))/pos.shape[0]))\n",
    "print ('# of NON spam containing \\\"%s\\\" = %d/%d = %0.2f%%'% \\\n",
    "    (most_common_word, sum(neg[:,1190]),neg.shape[0],      \\\n",
    "     100.*float(sum(neg[:,1190]))/neg.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
